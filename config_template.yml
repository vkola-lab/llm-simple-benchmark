# benchmarks results will be saved in this directory,
# in a directory with a unique name. The output will be one JSONL file per
# benchmark, with filename [benchmarkname]_output.jsonl
results_dir: 'results'

# Use guided decoding? If true, we expect an answer_schema.json file in the benchmark directory
use_guided_decoding: false

# how to do guided decoding, either 'xgrammar' or 'outlines'
# has no effect if use_guided_decoding is false
# xgrammar has some issues, sometimes it repeats the same token endlessly
# so it's recommended to use outlines, even if it's slower
guided_decoding_backend: 'outlines'

# model name to be passed to vllm.LLM, it should be eithe a local path or a
# huggingface path, make sure to set the environment variable HF_HOME to control
# where the model weights will be stored 
model_name : "Qwen/Qwen2.5-3B-Instruct"

# Number of GPUs used. Passed to vllm.LLM.tensor_parallel_size notice that SCC
# will only give us single nodes with multiple GPUs, so we never need to do
# multinode distributed inference
n_gpus: 1

# Fraction of gpu memory reserved for model + attention.  higher values will
# give higher throughput but risk giving out-of-memory errors when processing
# large amounts of text
gpu_memory_utilization: 0.9
max_model_len: 32768

# Memory amount to offload to cpu to simulate extra gpu memory. Slow but lets
# you load models larger than your gpu
cpu_offload_gb: 0

# seed for random sampling during text generation
llm_sampling_seed: 111

# Enforce eager execution? With eager execution the CUDA graph is not built
# beforehand, unclear which option has better performance
enforce_eager: false

# Chunked prefill improves inter-token latency and generation decode because #
# decode requests are prioritized. It helps achieve better GPU utilization by
# locating compute-bound (prefill) and memory-bound (decode) requests to the
# same batch.
enable_chunked_prefill: true

# LoRA
enable_lora: false
lora_path: 

system_prompt: "You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n...\n</think>\n<answer>\n...\n</answer>. Reply in English only, do not use other languages."

# sampling parameters
temperature: 0.7
min_p: 0 # minimum probability for a token to be considered, relative to most probable token. Set to 0 to disable
top_p: 0.95 # consider candidate tokens until the cumulative sum of their probabilities is top_p
min_tokens: 0 # minimum number of tokens to generate
max_new_tokens: 2048 # maximum number of tokens to generate
repetition_penalty: 1.0 # values larger than 1 discourage the use of tokens that have been already generated

# read at most this many questions from the benchmark, so it's easy to run a subset
max_questions: 1000

# number of sequences to generate per prompt
n: 5

# each item should be the path to a benchmark folder
# a benchmark folder must contain an answer schema if running with guided decoding
# the content of the benchmark should be in a JSONL file
benchmarks:
  - 'benchmarks/nacc_cognitive_1000_json'
  # - 'benchmarks/adrd_cog_status'
