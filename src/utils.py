from datetime import datetime
from omegaconf import OmegaConf
from pathlib import Path
import argparse
import json
import uuid


def make_results_dir(config):
    # make a directory for the run inside results_dir
    # inside the run directory make one dir per benchmark

    run_name = "_".join((datetime.now().strftime("%FT%H%M%S"), uuid.uuid4().hex[:16]))

    run_path = Path(config.results_dir) / run_name

    run_path.mkdir(parents=True, exist_ok=False)

    # save a copy of the current config file in the run directory so it is easy to rerun later
    OmegaConf.save(config, run_path / "config.yml")

    return run_path


def make_parser():

    parser = argparse.ArgumentParser(
        prog="LLM Benchmark Evaluator",
        description="Run an LLM on a collection of benchmarks.",
    )

    parser.add_argument(
        "config_file",
        default="config.yml",
        help="Path to configuration fire (default: config.yml)",
    )

    return parser


def outputs_to_dict(problems, outputs):
    # given an outputs object, like the one generated by llm.generate,
    # turn it into a dictionary ready to be saved to json
    # outputs is a list, one item per problem in the benchmark

    output_list = []

    for i in range(len(problems)):

        output_dict = {}

        output_dict["problem"] = problems[i] 
        output_dict["prompt"] = outputs[i].prompt
        output_dict["generated_text"] = [_.text for _ in outputs[i].outputs]
        # did the output reach the max number of tokens?
        output_dict["finish_reason"] = [_.finish_reason for _ in outputs[i].outputs]

        output_list.append(output_dict)

    return output_list


def save_results(run_path, benchmark, problems, outputs):

    benchmark_path = Path(benchmark)

    output_dicts = outputs_to_dict(problems, outputs)

    with open(run_path / (benchmark_path.stem + "_output.jsonl"), "w") as f:
        for output_dict in output_dicts:
            f.write(json.dumps(output_dict) + "\n")
